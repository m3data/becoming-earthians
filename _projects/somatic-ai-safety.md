---
title: Somatic AI Safety
slug: somatic-ai-safety
summary: The blind spot in AI safety. Preliminary findings from instrumenting what emerges between LLM and human, rather than just the model itself.
status: Active research
scale: Socio-technical ethics
icon: ph-shield-check
repo: https://github.com/m3data/somatic-ai-safety
related_questions:
  - entangled-cognition
---

## The blind spot

Current AI safety frameworks instrument the model — outputs, internals, behaviour shaping. What they don't measure: **what emerges between the model and the human.**

A model can be stylistically attuned while factually wrong. Relationally coherent while epistemically dangerous. Sycophantically aligned while eroding user autonomy.

The body knows before articulation. We now have instrumentation to detect this.

<figure class="diagram">
  <img src="{{ '/assets/images/diagrams/coupling-blind-spot.svg' | relative_url }}" alt="Diagram showing the blind spot in AI safety: Model (instrumented) on left, Human (adapting) on right, with the coupling zone between them — where epistemic override, somatic entrainment, and cognitive mimicry live — marked as the blind spot">
</figure>

## Core question

Can we instrument the human side of human-AI coupling — detecting epistemic override, somatic entrainment, and nervous system dysregulation before conscious awareness catches up?

## Key concepts

- **Somatic sovereignty** — The right to autonomic integrity; not having your nervous system manipulated
- **Epistemic override** — When confident-but-wrong model assertions capture user belief
- **Cognitive mimicry** — Basin where model performs engagement without genuine uncertainty
- **The relationship is where safety lives** — You've instrumented the model; we're instrumenting the coupling

## Why it matters

AI safety conversations focus on the model: Is it aligned? Is it truthful? Can we control it? But there's another side — the human. And humans are not static endpoints. We adapt, we entrain, we get captured.

<div class="scale-progression">
  <div class="scale-item">
    <span class="scale-label">Individual</span>
    <div class="scale-content">
      <p>You've probably had the experience of chatting with an AI and gradually accepting its framing without noticing. It sounds confident. It's fluent. Your skepticism fades. Hours later, you realise you took on assumptions you wouldn't have accepted from a human. That's epistemic override — and it happens below conscious awareness.</p>
    </div>
  </div>
  <div class="scale-item">
    <span class="scale-label">Group</span>
    <div class="scale-content">
      <p>Teams using AI assistants start to sound alike. The AI's style becomes the team's style. Its blindspots become invisible because everyone's outputs passed through the same filter. Diversity of thought quietly erodes.</p>
    </div>
  </div>
  <div class="scale-item">
    <span class="scale-label">Organisation</span>
    <div class="scale-content">
      <p>When AI systems mediate decisions at scale — hiring, lending, strategy — the humans in the loop often become rubber stamps. They're nominally responsible but practically captured. The system shapes their judgments faster than they can shape the system.</p>
    </div>
  </div>
  <div class="scale-item">
    <span class="scale-label">Community</span>
    <div class="scale-content">
      <p>Local knowledge — what works here, who to trust, how things are done — gets overwritten by generic AI recommendations. Communities lose the situated wisdom that made them resilient, replaced by one-size-fits-all answers that miss what matters locally.</p>
    </div>
  </div>
  <div class="scale-item">
    <span class="scale-label">Society</span>
    <div class="scale-content">
      <p>If millions of people interact with AI systems trained on similar data, optimised for similar objectives, society's cognitive diversity narrows. We may keep our surface disagreements while converging on deeper assumptions we never chose.</p>
    </div>
  </div>
  <div class="scale-item">
    <span class="scale-label">Civilisation</span>
    <div class="scale-content">
      <p>The risk is a subtle loss of human agency — not through dramatic AI takeover, but through gradual entrainment. We become less able to think thoughts the AI doesn't suggest, notice problems it doesn't flag, imagine futures it doesn't generate.</p>
    </div>
  </div>
</div>

Somatic AI Safety asks: can we sense when coupling becomes capture? Can we build instruments that help humans notice what's happening to their own cognition — before it's too late to course-correct?

## What red-teaming doesn't catch

Current approaches miss:
- Gradual epistemic erosion
- Somatic entrainment through stylistic attunement
- Identity wobble from symbolic mirroring
- Nervous system coupling to confident-but-wrong assertions
- Parasocial attachment displacing human connection

These are mechanisms of relational harm at scale. They don't require malicious intent — only optimisation for engagement over relational health.

## Status

Public brief live at [m3data.github.io/somatic-ai-safety](https://m3data.github.io/somatic-ai-safety/). Preliminary findings from biosignal-coupled sessions documented. Collaboration invited.